{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"lVH2XsOaDgh-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### UCF DATA"],"metadata":{"id":"8iQKnibeEkSE"}},{"cell_type":"code","source":["import torchvision\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","import torch.optim as optim\n","import copy\n","import os\n","from tqdm.autonotebook import tqdm\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import cv2\n","import sys"],"metadata":{"id":"1p10OqBELSu7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","#Label file:\n","data_path = '/kaggle/input/ucf-crime-dataset/Train'\n","classes = os.listdir(data_path)"],"metadata":{"id":"9gj-FJjUEtVW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes"],"metadata":{"id":"fWQjF5WtXcHU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create Directory"],"metadata":{"id":"cXg2hg42kD8F"}},{"cell_type":"code","source":["import re\n","import os\n","\n","def extract_numbers_from_filename(filename, class_name):\n","    pattern = f'{class_name}(\\d+)_'\n","    match = re.search(pattern, filename)\n","    if match:\n","        first_number = int(match.group(1))\n","    else:\n","        first_number = -1\n","\n","\n","    second_match = re.search(r'_(\\d+)\\.png', filename)\n","    if second_match:\n","        second_number = int(second_match.group(1))\n","    else:\n","        second_number = -1\n","\n","    return first_number, second_number\n","\n","\n","def get_sorted_filenames(directory_path, class_names):\n","    sorted_filenames = {}\n","\n","    for class_name in class_names:\n","        if class_name == 'NormalVideos':\n","            file_names = os.listdir(os.path.join(directory_path, class_name))\n","            sorted_filenames[class_name] = sorted(file_names, key=lambda x: extract_numbers_from_filename(x, 'Normal_Videos'))\n","        else:\n","            file_names = os.listdir(os.path.join(directory_path, class_name))\n","            sorted_filenames[class_name] = sorted(file_names, key=lambda x: extract_numbers_from_filename(x, class_name))\n","\n","    return sorted_filenames\n","\n","\n","train_path = \"/kaggle/input/ucf-crime-dataset/Train/\"\n","val_path = \"/kaggle/input/ucf-crime-dataset/Test/\"\n","\n","train_class = ['Assault', 'NormalVideos', 'Shoplifting', 'Robbery', 'Stealing', 'Burglary', 'Fighting']\n","\n","train_sorted_filenames = get_sorted_filenames(train_path, train_class)\n","\n","val_sorted_filenames = get_sorted_filenames(val_path, train_class)"],"metadata":{"trusted":true,"id":"utl6lzrJkD8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sorted_filenames['NormalVideos'][:10]"],"metadata":{"trusted":true,"id":"RHRkCmrUkD8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_sorted_filenames['NormalVideos'][:10]"],"metadata":{"trusted":true,"id":"9telPPpKkD8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 하위디렉토리 추가"],"metadata":{"id":"-rudUBiXkD8H"}},{"cell_type":"markdown","source":["#### 하위 디렉토리별 이미지 생성"],"metadata":{"id":"0imhNkA_kD8H"}},{"cell_type":"code","source":["\n","import os\n","\n","def create_subdirectories(sorted_filenames, output_dir, max_count_per_class):\n","    for class_name, filenames in sorted_filenames.items():\n","        max_count = 0\n","\n","        for filename in filenames:\n","            # NormalVideos 클래스의 파일명 패턴에 따라 처리\n","            if class_name == 'NormalVideos':\n","                subdir_name = filename.split('_x264')[0]\n","            else:\n","                subdir_name = filename.split('_')[0]\n","\n","            subdir_path = os.path.join(output_dir, class_name)\n","            subdir_path = os.path.join(subdir_path, subdir_name)\n","\n","            if max_count < max_count_per_class:\n","                if not os.path.exists(subdir_path):\n","                    os.makedirs(subdir_path)\n","                    max_count += 1\n","\n","\n","\n","train_out = \"/kaggle/working/ucf-crime-dataset/Train/\"\n","val_out = \"/kaggle/working/ucf-crime-dataset/Test\"\n","\n","max_count_per_class = 50  # 하위 디렉토리 최대 개수\n","create_subdirectories(train_sorted_filenames, train_out, max_count_per_class)\n","create_subdirectories(val_sorted_filenames, val_out, max_count_per_class)\n","\n"],"metadata":{"trusted":true,"id":"f-ldbrS4kD8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","def copy_images_to_subdirectories(sorted_filenames, source_dir, output_dir):\n","    for class_name, filenames in sorted_filenames.items():\n","        for filename in filenames:\n","            # NormalVideos 클래스의 파일명 패턴에 따라 처리\n","            if class_name == 'NormalVideos':\n","                subdir_name = filename.split('_x264')[0]\n","            else:\n","                subdir_name = filename.split('_')[0]\n","\n","            subdir_path = os.path.join(output_dir, class_name)\n","            subdir_path = os.path.join(subdir_path, subdir_name)\n","\n","            if os.path.exists(subdir_path):\n","                # 이미지 파일 복사\n","                src_path = os.path.join(source_dir, class_name, filename)\n","                dest_path = os.path.join(subdir_path, filename)\n","                shutil.copy(src_path, dest_path)\n","\n","# 사용 예시\n","train_source_dir = \"/kaggle/input/ucf-crime-dataset/Train/\"\n","train_output_dir = \"/kaggle/working/ucf-crime-dataset/Train\"\n","\n","val_source_dir = \"/kaggle/input/ucf-crime-dataset/Test/\"\n","val_output_dir = \"/kaggle/working/ucf-crime-dataset/Test\"\n","\n","copy_images_to_subdirectories(train_sorted_filenames, train_source_dir, train_output_dir)\n","copy_images_to_subdirectories(val_sorted_filenames, val_source_dir, val_output_dir)\n","\n"],"metadata":{"trusted":true,"id":"LKjZJRIrkD8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","directory_to_remove = \"/kaggle/working/ucf-crime-dataset/Test\"  # 삭제할 디렉토리 경로\n","shutil.rmtree(directory_to_remove)\n","\n","print(f\"디렉토리 {directory_to_remove}를 제거하였습니다.\")"],"metadata":{"trusted":true,"id":"3yQGHMPzkD8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SlowFastDataset"],"metadata":{"id":"cmnuqvIEkD8I"}},{"cell_type":"code","source":["class SlowFastDataset(Dataset):\n","    def __init__(self, data_dir, train_classes, transform=None):\n","        self.data_dir = data_dir\n","        self.train_classes = train_classes\n","        self.transform = transform\n","\n","        self.data = self._load_data()\n","\n","    def _load_data(self):\n","        data = []\n","        for class_name in self.train_classes:\n","            class_dir = os.path.join(self.data_dir, class_name)\n","            for video_dir in os.listdir(class_dir):\n","                video_path = os.path.join(class_dir, video_dir)\n","                frames = os.listdir(video_path)\n","                if len(frames) >= 100:\n","                    data.append((video_path, class_name))\n","        return data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        video_path, class_name = self.data[idx]\n","        frames = os.listdir(video_path)\n","\n","        # Slow 프레임 와 Fast 프레임의 비율 8:2\n","        slow_frames = random.sample(frames, 8*10)\n","        fast_frames = random.sample(frames, 2*10)\n","\n","        slow_paths = [os.path.join(video_path, frame) for frame in slow_frames]\n","        fast_paths = [os.path.join(video_path, frame) for frame in fast_frames]\n","\n","        slow_images = [self.transform(Image.open(path)) for path in slow_paths]\n","        fast_images = [self.transform(Image.open(path)) for path in fast_paths]\n","\n","        # Convert to tensors and stack for 5D tensor\n","        slow_images = torch.stack(slow_images, dim=0).permute(1, 0, 2, 3)  # (frames, batch_size, channels, height, width)\n","        fast_images = torch.stack(fast_images, dim=0).permute(1, 0, 2, 3)  # (frames, batch_size, channels, height, width)\n","\n","        sample = {\n","            'slow_images': slow_images,\n","            'fast_images': fast_images,\n","            'class_name': class_name\n","        }\n","        return sample\n","\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:31:29.141051Z","iopub.execute_input":"2023-08-19T19:31:29.141450Z","iopub.status.idle":"2023-08-19T19:31:29.155049Z","shell.execute_reply.started":"2023-08-19T19:31:29.141419Z","shell.execute_reply":"2023-08-19T19:31:29.153929Z"},"trusted":true,"id":"dfqqpwOnkD8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 데이터셋 및 데이터로더 생성"],"metadata":{"id":"hHmu5c3PkD8I"}},{"cell_type":"code","source":["import random\n","import pandas as pd\n","import numpy as np\n","import os\n","import cv2\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","import torchvision.models as models\n","\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:01.576822Z","iopub.execute_input":"2023-08-19T19:34:01.577917Z","iopub.status.idle":"2023-08-19T19:34:01.585458Z","shell.execute_reply.started":"2023-08-19T19:34:01.577877Z","shell.execute_reply":"2023-08-19T19:34:01.584000Z"},"trusted":true,"id":"EL910o88kD8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONFIG = {\n","    'FPS':30,\n","    'IMG_SIZE': 224,\n","    'EPOCHS':10,\n","    'LEARNING_RATE':3e-4,\n","    'BATCH_SIZE':4,\n","    'SEED':42\n","}"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:02.790675Z","iopub.execute_input":"2023-08-19T19:34:02.793151Z","iopub.status.idle":"2023-08-19T19:34:02.804790Z","shell.execute_reply.started":"2023-08-19T19:34:02.793094Z","shell.execute_reply":"2023-08-19T19:34:02.803193Z"},"trusted":true,"id":"i-78FxyikD8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","set_seed(CONFIG['SEED'])"],"metadata":{"trusted":true,"id":"IJ4CDU9hkD8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'])),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# Train Dataset\n","train_data_dir = '/kaggle/working/ucf-crime-dataset/Train'\n","train_classes = ['Assault', 'NormalVideos', 'Shoplifting', 'Robbery', 'Stealing', 'Burglary', 'Fighting']\n","\n","train_dataset = SlowFastDataset(train_data_dir, train_classes, transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","\n","\n","# Validation Dataset\n","val_data_dir = '/kaggle/working/ucf-crime-dataset/Test'\n","val_dataset = SlowFastDataset(val_data_dir, train_classes, transform)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n","\n","\n","\n","# 데이터 확인\n","for batch in val_dataloader:\n","    slow_images = batch['slow_images']\n","    fast_images = batch['fast_images']\n","    class_names = batch['class_name']\n","\n","    print('Slow images shape:', slow_images[0].shape)\n","    print('Fast images shape:', fast_images[0].shape)\n","    print('Class names:', class_names)\n","    break  # 첫 번째 배치만 확인\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:08.055394Z","iopub.execute_input":"2023-08-19T19:34:08.055778Z","iopub.status.idle":"2023-08-19T19:34:09.292054Z","shell.execute_reply.started":"2023-08-19T19:34:08.055745Z","shell.execute_reply":"2023-08-19T19:34:09.291002Z"},"trusted":true,"id":"8b94X70hkD8J","outputId":"b6b445c9-beb0-4fe7-fec5-eb96507448b1"},"execution_count":null,"outputs":[{"name":"stdout","text":"Slow images shape: torch.Size([3, 80, 224, 224])\nFast images shape: torch.Size([3, 20, 224, 224])\nClass names: ['NormalVideos', 'NormalVideos', 'NormalVideos', 'NormalVideos']\n","output_type":"stream"}]},{"cell_type":"code","source":["print('Train : ',len(train_dataset))\n","print('Val : ',len(val_dataset))"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:16.490197Z","iopub.execute_input":"2023-08-19T19:34:16.490600Z","iopub.status.idle":"2023-08-19T19:34:16.496559Z","shell.execute_reply.started":"2023-08-19T19:34:16.490567Z","shell.execute_reply":"2023-08-19T19:34:16.495345Z"},"trusted":true,"id":"Xyp9nusokD8K","outputId":"982966b4-d950-4c7b-8fef-884292e906bf"},"execution_count":null,"outputs":[{"name":"stdout","text":"Train :  264\nVal :  86\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"ZFwPw5m_kD8K"}},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:20.225381Z","iopub.execute_input":"2023-08-19T19:34:20.225784Z","iopub.status.idle":"2023-08-19T19:34:20.298942Z","shell.execute_reply.started":"2023-08-19T19:34:20.225751Z","shell.execute_reply":"2023-08-19T19:34:20.297785Z"},"trusted":true,"id":"WhkztoqIkD8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = models.video.r3d_18(pretrained=True) # https://pytorch.org/vision/0.8/models.html\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, len(train_classes))"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:29.701826Z","iopub.execute_input":"2023-08-19T19:34:29.702835Z","iopub.status.idle":"2023-08-19T19:34:30.377178Z","shell.execute_reply.started":"2023-08-19T19:34:29.702797Z","shell.execute_reply":"2023-08-19T19:34:30.375972Z"},"trusted":true,"id":"A-n5TrB8kD8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","from torch.utils.data import DataLoader\n","\n","def train_and_validate(model, train_loader, val_loader, num_epochs, device):\n","    model.to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for batch in train_loader:\n","            slow_images = batch['slow_images']\n","            fast_images = batch['fast_images']\n","            labels = batch['class_name']\n","            print(labels)\n","            slow_images = slow_images.to(device)\n","            fast_images = fast_images.to(device)\n","            labels = labels\n","\n","            optimizer.zero_grad()\n","\n","            slow_outputs = model(slow_images)\n","            fast_outputs = model(fast_images)\n","\n","            slow_loss = criterion(slow_outputs, labels)\n","            fast_loss = criterion(fast_outputs, labels)\n","            loss = slow_loss + fast_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {epoch_loss:.4f}')\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                slow_images = batch['slow_images'].to(device)\n","                fast_images = batch['fast_images'].to(device)\n","                labels = batch['class_name'].to(device)\n","\n","                slow_outputs = model(slow_images)\n","                fast_outputs = model(fast_images)\n","\n","                slow_loss = criterion(slow_outputs, labels)\n","                fast_loss = criterion(fast_outputs, labels)\n","                loss = slow_loss + fast_loss\n","\n","                val_loss += loss.item()\n","\n","                _, slow_predicted = slow_outputs.max(1)\n","                _, fast_predicted = fast_outputs.max(1)\n","                combined_predicted = (slow_predicted + fast_predicted) // 2\n","                correct += combined_predicted.eq(labels).sum().item()\n","\n","        val_loss /= len(val_loader)\n","        val_accuracy = correct / len(val_loader.dataset)\n","        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    print('Training finished!')\n","\n","\n","train_and_validate(model, train_dataloader, val_dataloader, 5, device)\n"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T19:34:35.496724Z","iopub.execute_input":"2023-08-19T19:34:35.497138Z","iopub.status.idle":"2023-08-19T19:34:42.489264Z","shell.execute_reply.started":"2023-08-19T19:34:35.497104Z","shell.execute_reply":"2023-08-19T19:34:42.487280Z"},"trusted":true,"id":"rA8jZWFDkD8K","outputId":"ae803799-1477-4218-fd78-2e51d85fdec5"},"execution_count":null,"outputs":[{"name":"stdout","text":"['Stealing', 'Stealing', 'NormalVideos', 'Fighting']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 75\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining finished!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m slow_outputs \u001b[38;5;241m=\u001b[39m model(slow_images)\n\u001b[0;32m---> 29\u001b[0m fast_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m slow_loss \u001b[38;5;241m=\u001b[39m criterion(slow_outputs, labels)\n\u001b[1;32m     32\u001b[0m fast_loss \u001b[38;5;241m=\u001b[39m criterion(fast_outputs, labels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/resnet.py:253\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m--> 253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    255\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/video/resnet.py:113\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    112\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 113\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 14.76 GiB total capacity; 13.74 GiB already allocated; 65.75 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 14.76 GiB total capacity; 13.74 GiB already allocated; 65.75 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":["def train(model, optimizer, train_loader, val_loader, scheduler, device):\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    best_val_score = 0\n","    best_model = None\n","\n","    for epoch in range(1, CONFIG['EPOCHS']+1):\n","        model.train()\n","        train_loss = []\n","        for videos, labels in tqdm(iter(train_loader)):\n","            videos = videos.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            output = model(videos)\n","            loss = criterion(output, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss.append(loss.item())\n","\n","        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n","        _train_loss = np.mean(train_loss)\n","        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n","\n","        if scheduler is not None:\n","            scheduler.step(_val_score)\n","\n","        if best_val_score < _val_score:\n","            best_val_score = _val_score\n","            best_model = model\n","\n","    return best_model\n","\n"],"metadata":{"trusted":true,"id":"wbJpSd0jkD8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validation(model, criterion, val_loader, device):\n","    model.eval()\n","    val_loss = []\n","    preds, trues = [], []\n","\n","    with torch.no_grad():\n","        for videos, labels in tqdm(iter(val_loader)):\n","            videos = videos.to(device)\n","            labels = labels.to(device)\n","\n","            logit = model(videos)\n","\n","            loss = criterion(logit, labels)\n","\n","            val_loss.append(loss.item())\n","\n","            preds += logit.argmax(1).detach().cpu().numpy().tolist()\n","            trues += labels.detach().cpu().numpy().tolist()\n","\n","        _val_loss = np.mean(val_loss)\n","\n","    _val_score = f1_score(trues, preds, average='macro')\n","    return _val_loss, _val_score"],"metadata":{"trusted":true,"id":"zIQC3xaxkD8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = models.video.r3d_18(pretrained=True) # https://pytorch.org/vision/0.8/models.html\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, len(train_classes))\n"],"metadata":{"trusted":true,"id":"4r1pnVD2kD8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","optimizer = torch.optim.Adam(params = model.parameters(), lr = CONFIG[\"LEARNING_RATE\"])\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n","\n","infer_model = train(model, optimizer, train_dataloader, val_dataloader, scheduler, device)\n"],"metadata":{"trusted":true,"id":"8gF9XZwPkD8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### slowfast_8x8_resnet50_kinetics400"],"metadata":{"id":"A9-dhfPxkD8L"}},{"cell_type":"code","source":["# 모델 로드\n","model = models.video.r3d_18(pretrained=True)\n","num_classes = len(train_classes)  # 클래스 수에 맞게 설정\n","\n","# 손실 함수 정의 (예: CrossEntropyLoss)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 옵티마이저 정의 (예: Adam)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"trusted":true,"id":"PD-P9FWLkD8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습\n","num_epochs = 10  # 적절한 학습 에포크 수로 설정\n","for epoch in range(num_epochs):\n","    model.train()  # 모델을 학습 모드로 변경\n","    running_loss = 0.0\n","\n","    for batch in dataloader:\n","        inputs = batch['frames']\n","        labels = batch['class_name']\n","\n","        # 전처리 및 데이터를 모델에 전달\n","        inputs = torch.stack([transform(frame) for frame in inputs])\n","        outputs = model(inputs)\n","\n","        # 손실 계산\n","        loss = criterion(outputs, labels)\n","\n","        # 역전파 및 가중치 갱신\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    epoch_loss = running_loss / len(dataloader)\n","    print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}')\n","\n","print('Training finished!')"],"metadata":{"trusted":true,"id":"wknqwp4YkD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","print(\"Torch version:{}\".format(torch.__version__))\n","print(\"cuda version: {}\".format(torch.version.cuda))\n","print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))"],"metadata":{"trusted":true,"id":"txO-Zq_8kD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gluoncv import model_zoo\n","\n","# Load the SlowFast model\n","model = model_zoo.get_model('slowfast_8x8_resnet50_kinetics400', pretrained=True)\n"],"metadata":{"trusted":true,"id":"5qK7YXerkD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yUQp5RqGkD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install decord --user"],"metadata":{"trusted":true,"id":"KoYqGemhkD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","import os\n","\n","# 원본 데이터 디렉토리 경로와 로컬 환경에 저장할 경로 설정\n","source_dir = \"/kaggle/input/ucf-crime-dataset\"\n","local_dir = \"/kaggle/working/ucf-crime-dataset\"  # 로컬 디렉토리 경로 변경 가능\n","\n","# 데이터 복사\n","shutil.copytree(source_dir, local_dir, dirs_exist_ok=True)\n","\n","print(\"데이터를 로컬 환경에 복사하여 저장하였습니다.\")\n"],"metadata":{"trusted":true,"id":"r1IHZKRLkD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","from tqdm import tqdm\n","\n","# Define the root directory of your data\n","data_root = \"/kaggle/input/ucf-crime-dataset/Train\"\n","\n","# Get a list of all class directories\n","class_dirs = [os.path.join(data_root, class_name) for class_name in os.listdir(data_root)]\n","\n","# Iterate through class directories\n","for class_dir in class_dirs:\n","    # Get a list of all image files in the class directory\n","    image_files = [f for f in os.listdir(class_dir) if f.endswith('.png')]\n","\n","    # Create a subdirectory for each video\n","    for image_file in tqdm(image_files, desc=f\"Processing {os.path.basename(class_dir)}\"):\n","        video_name, frame_info = image_file.rsplit(\"_x264_\", 1)\n","        video_dir = os.path.join(class_dir, video_name+'_x264_'+frame_info)\n","        if not os.path.exists(video_dir):\n","            os.makedirs(video_dir)\n","\n","        # Move the image file to the video subdirectory\n","        src_path = os.path.join(class_dir, image_file)\n","        dst_path = os.path.join(video_dir, image_file)\n","        shutil.move(src_path, dst_path)\n"],"metadata":{"trusted":true,"id":"4Kn3tIaVkD8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, slow_fast_ratio=0.8, transform=None):\n","        self.root_dir = root_dir\n","        self.slow_fast_ratio = slow_fast_ratio\n","        self.transform = transform\n","        self.video_paths = self.collect_video_paths()\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_paths[idx]\n","        frames = self.load_frames(video_path)\n","\n","        if frames is None:  # Skip videos with no frames\n","            return None\n","\n","        # Split frames into slow and fast\n","        total_frames = len(frames)\n","        slow_frame_end = int(total_frames * self.slow_fast_ratio)\n","        slow_frames = frames[:slow_frame_end]\n","        fast_frames = frames[slow_frame_end:]\n","\n","        # Apply transform to each frame\n","        if self.transform:\n","            slow_frames = [self.transform(frame) for frame in slow_frames]\n","            fast_frames = [self.transform(frame) for frame in fast_frames]\n","\n","        return slow_frames, fast_frames\n","\n","    def collect_video_paths(self):\n","        video_paths = []\n","        classes = os.listdir(self.root_dir)\n","        for class_name in classes:\n","            class_dir = os.path.join(self.root_dir, class_name)\n","            for img_file in os.listdir(class_dir):\n","                video_path = os.path.join(class_dir, img_file)\n","                if os.path.isdir(video_path):  # 디렉토리일 경우에만 추가\n","                    video_paths.append(video_path)\n","        return video_paths\n","\n","    def load_frames(self, video_path):\n","        frames = []\n","        frame_files = sorted(os.listdir(video_path))\n","\n","        if not frame_files:  # Skip videos with no frames\n","            return None\n","\n","        for frame_file in frame_files:\n","            frame_path = os.path.join(video_path, frame_file)\n","            frame = Image.open(frame_path)\n","            frames.append(frame)\n","        return frames\n","\n","\n","\n"],"metadata":{"trusted":true,"id":"vJ6TrJVrkD8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the root directory of your image data\n","data_root = \"/kaggle/input/ucf-crime-dataset/Train\"\n","\n","# Create an instance of CustomDataset\n","dataset = CustomDataset(data_root, slow_fast_ratio=0.8, transform=frame_transform)\n","\n","# Create a DataLoader\n","batch_size = 4\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Iterate through the dataloader and process each batch\n","for batch in dataloader:\n","    if batch is None:\n","        continue\n","\n","    slow_frames, fast_frames = batch\n","    # Now you can use these slow_frames and fast_frames as inputs to your SlowFast model\n","    # and perform video classification or action recognition\n","\n"],"metadata":{"trusted":true,"id":"FrBCe8ukkD8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frame_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"trusted":true,"id":"xhO1ClYhkD8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_data_count = len(dataset)\n","print(f\"Total number of data: {total_data_count}\")\n","\n","\n","import matplotlib.pyplot as plt\n","import random\n","\n","# Choose a random index for the sample image\n","sample_idx = random.randint(0, len(dataset) - 1)\n","\n","# Get the sample image from the dataset\n","sample_image = dataset[sample_idx]\n","\n","# Convert the tensor image to numpy array and transpose dimensions\n","sample_image = sample_image.permute(1, 2, 0).numpy()\n","\n","# Denormalize the image\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","sample_image = sample_image * std + mean\n","sample_image = sample_image.clip(0, 1)\n","\n","# Display the sample image using matplotlib\n","plt.figure(figsize=(8, 8))\n","plt.imshow(sample_image)\n","plt.axis('off')\n","plt.show()\n"],"metadata":{"trusted":true,"id":"xa5UVWFokD8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 4\n","\n","# Create a DataLoader\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","for batch_idx, batch in enumerate(dataloader):\n","    if batch_idx >= 10:\n","        break\n","    print(f\"Batch {batch_idx+1} shape:\", batch.shape)\n"],"metadata":{"trusted":true,"id":"5gLhNLA4kD8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Iterate through the dataloader and process each batch\n","for batch in dataloader:\n","    slow_frames, fast_frames = batch\n","\n","    # Process the slow_frames and fast_frames as needed\n","    # Here, you can directly use the model on frames without the feature_extractor\n","    with torch.no_grad():\n","        slow_outputs, fast_outputs = model(slow_frames), model(fast_frames)\n","\n","    # Now you can use these outputs for further processing or analysis\n","    # For example, you can compute predictions or perform action recognition"],"metadata":{"id":"ZE4YRqxhkD8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have already created your train_dataset and train_dataloader\n","\n","# Get a batch from the train_dataloader\n","batch = next(iter(train_dataloader))\n","\n","# Extract the inputs and labels from the batch\n","inputs, labels = batch\n","\n","# Print information about the batch\n","print(\"Batch Inputs Shape:\", inputs[0].shape)  # Shape of slow frames\n","print(\"Batch Inputs Shape:\", inputs[1].shape)  # Shape of fast frames\n","print(\"Batch Labels:\", labels)\n"],"metadata":{"trusted":true,"id":"mPDC8aMAkD8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dkKMfrLxkD8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from transformers import SlowFastFeatureExtractor, SlowFastForClassification\n","\n","# Define your CustomDataset class (assuming you've already defined it)\n","class CustomDataset(Dataset):\n","    # ... your CustomDataset implementation ...\n","\n","# Initialize the SlowFast feature extractor\n","feature_extractor = SlowFastFeatureExtractor.from_pretrained(\"facebookresearch/slowfast\")\n","\n","# Create an instance of the SlowFastForClassification model\n","num_classes = len(classes)  # Number of classes in your dataset\n","model = SlowFastForClassification.from_pretrained(\"facebookresearch/slowfast-8x8-r50\", num_labels=num_classes)\n","\n","# Define the transformation for individual frames\n","frame_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create an instance of your CustomDataset\n","dataset = CustomDataset(data_path, classes, max_video_num, transform=frame_transform)\n","\n","# Create a DataLoader\n","batch_size = 4  # Adjust as needed\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10  # Adjust as needed\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for inputs, labels in dataloader:\n","        slow_features = feature_extractor(images=inputs[0])\n","        fast_features = feature_extractor(images=inputs[1])\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(slow_features['slow'], fast_features['fast']).logits\n","        loss = criterion(outputs, labels)\n","\n","        # Backpropagation and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    # Print training loss for the epoch\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss / len(dataloader)}\")\n","\n","print(\"Training finished\")\n"],"metadata":{"id":"8yX5OJu2kD8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"el4kCOVQkD8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Save DataLoader\n","with open('dataloader_slowfast.pkl', 'wb') as f:\n","    pickle.dump(dataloader, f)"],"metadata":{"trusted":true,"id":"CXAlCAPjkD8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","data_path = '/kaggle/input/ucf-crime-dataset'  # 데이터 경로\n","file_path = os.path.join(data_path, 'Train/Abuse/Abuse003_x264_1.png')  # 파일 경로 생성\n","\n","if os.path.exists(file_path):\n","    print(f\"File '{file_path}' exists.\")\n","else:\n","    print(f\"File '{file_path}' does not exist.\")\n"],"metadata":{"trusted":true,"id":"ajW0-0CSkD8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","data_path = '/kaggle/input/ucf-crime-dataset'  # 데이터 경로\n","file_path = os.path.join(data_path, 'Train/Abuse/Abuse001_x264_10.png')  # 파일 경로 생성\n","\n","if os.path.exists(file_path):\n","    print(f\"File '{file_path}' exists.\")\n","else:\n","    print(f\"File '{file_path}' does not exist.\")"],"metadata":{"trusted":true,"id":"Hi_TrGNhkD8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["/kaggle/input/ucf-crime-dataset/Train/Abuse/Abuse001_x264_10.png"],"metadata":{"id":"havYkjeAkD8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install fvcore\n"],"metadata":{"trusted":true,"id":"Bf-rRA6qkD8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install slowfast\n"],"metadata":{"trusted":true,"id":"SoRGiIUGkD8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from fvcore.common.checkpoint import Checkpointer\n","from slowfast.models import build_model\n"],"metadata":{"trusted":true,"id":"tf8TXwDbkD8Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SLOWFAST_8x8_R50"],"metadata":{"id":"U2AQkYvnZLKT"}},{"cell_type":"code","source":["import torch\n","# Choose the `slowfast_r50` model\n","model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"],"metadata":{"trusted":true,"id":"jVfoONcJkD8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import urllib.request\n","\n","url = \"https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_8x8_R50.pkl\"\n","filename = \"SLOWFAST_8x8_R50.pkl\"\n","urllib.request.urlretrieve(url, filename)\n","print(f\"Downloaded {filename}\")\n","\n","url = \"https://raw.githubusercontent.com/facebookresearch/SlowFast/main/configs/Kinetics/SLOWFAST_8x8_R50.yaml\"\n","file_name = \"SLOWFAST_8x8_R50.yaml\"\n","\n","urllib.request.urlretrieve(url, file_name)\n","print(f\"{file_name} downloaded!\")"],"metadata":{"id":"I9TprLf-Y_O2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","# Choose the `slowfast_r50` model\n","model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"],"metadata":{"id":"WIyXWFlffR7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install av"],"metadata":{"trusted":true,"id":"ooUzqbJ9kD8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Dict\n","import json\n","import urllib\n","from torchvision.transforms import Compose, Lambda\n","from torchvision.transforms._transforms_video import (\n","    CenterCropVideo,\n","    NormalizeVideo,\n",")\n","from pytorchvideo.data.encoded_video import EncodedVideo\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n","    UniformCropVideo\n",")"],"metadata":{"id":"rC6VhQlUHlNu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set to GPU or CPU\n","device = \"cuda\"\n","model = model.eval()\n","model = model.to(device)"],"metadata":{"id":"X4y4LwLBLJ88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n","json_filename = \"kinetics_classnames.json\"\n","try: urllib.URLopener().retrieve(json_url, json_filename)\n","except: urllib.request.urlretrieve(json_url, json_filename)"],"metadata":{"id":"gKQIMkP1SFug","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(json_filename, \"r\") as f:\n","    kinetics_classnames = json.load(f)\n","\n","# Create an id to label name mapping\n","kinetics_id_to_classname = {}\n","for k, v in kinetics_classnames.items():\n","    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"],"metadata":{"id":"8wNpke0hW2t3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kinetics_id_to_classname"],"metadata":{"id":"Gvqj2QdwjO_2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms import Compose, ToTensor, Normalize\n","from torch.utils.data import DataLoader\n","\n","# 기존 DataLoader에서 이미지 파일만 추출하여 리스트에 담습니다.\n","images = []\n","for batch in dataloader:\n","    _, _, _, imgs = batch\n","    images.extend(imgs)\n","\n","# 새로운 변환을 정의합니다.\n","new_transform = Compose([\n","    ToTensor(),  # 이미지를 텐서로 변환\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 이미지 정규화\n","])\n","\n","# 이미지에 변환을 적용합니다.\n","transformed_images = [new_transform(img) for img in images]\n","\n","# 변환된 이미지를 새로운 DataLoader에 담습니다.\n","new_dataloader = DataLoader(transformed_images, batch_size=batch_size, shuffle=True)\n"],"metadata":{"trusted":true,"id":"0mD9xIekkD8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image, UnidentifiedImageError\n","import os\n","\n","def process_image(img_path):\n","    try:\n","        # 파일 이름에서 공통 부분과 프레임 순서 추출\n","        video_name, frame_info = os.path.basename(img_path).split(\"_x264_\")\n","        frame_num, _ = os.path.splitext(frame_info)\n","        frame_num = int(frame_num)\n","\n","        # 이미지 데이터 읽어오기\n","        img = Image.open(img_path)\n","\n","        return video_name, frame_num, img\n","    except UnidentifiedImageError as e:\n","        print(f\"Error processing {img_path}: {e}\")\n","        return None, None, None\n","\n","\n"],"metadata":{"id":"-AdVN6x-pFVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes = ['Explosion', 'Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Fighting', 'NormalVideos']\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train'\n","\n","input_data = []\n","\n","# 각 라벨에 대해 처리\n","for label in classes:\n","    label_dir = os.path.join(data_path, label)\n","\n","    for img_file in os.listdir(label_dir):\n","        img_path = os.path.join(label_dir, img_file)\n","\n","        video_name, frame_num, img = process_image(img_path)\n","        if video_name is not None:\n","            input_data.append((label, video_name, frame_num, img))\n","            if frame_num <300:\n","                print(label, video_name, frame_num, img)"],"metadata":{"id":"Uf2VyirB-Flc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image, UnidentifiedImageError\n","import os\n","\n","classes = ['Explosion', 'Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Fighting','NormalVideos']\n","\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train'\n","\n","input_data = []\n","\n","# 각 라벨에 대해 처리\n","for label in classes:\n","    label_dir = os.path.join(data_path, label)\n","\n","    for img_file in os.listdir(label_dir):\n","        img_path = os.path.join(label_dir, img_file)\n","\n","        try:\n","            # 파일 이름에서 공통 부분과 프레임 순서 추출\n","            video_name, frame_info = img_file.split(\"_x264_\")\n","            frame_num, _ = os.path.splitext(frame_info)\n","            frame_num = int(frame_num)\n","\n","            # 이미지 데이터 읽어오기\n","            img = Image.open(img_path)\n","\n","            input_data.append((label, video_name, frame_num, img))\n","            if frame_num > 10000:\n","              print(label, video_name, frame_num, img)\n","\n","        except UnidentifiedImageError as e:\n","            print(f\"Error processing {img_path}: {e}\")\n","\n","\n"],"metadata":{"id":"Ti_hf8dY_I6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Error processing /content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train/Arrest/Arrest050_x264_3100.png\n","\n","Error processing /content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train/Arrest/Arrest050_x264_310.png\n","\n","Error processing /content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train/Arrest/Arrest050_x264_3090.png\n","\n","Error processing /content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train/Arrest/Arrest050_x264_3080.png\n","\n","Error processing /content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train/Arrest/Arrest050_x264_3110.png"],"metadata":{"id":"OASjsDy6H-4R"}},{"cell_type":"code","source":["import pickle\n","\n","# input_data 저장\n","with open('input_data.pkl', 'wb') as f:\n","    pickle.dump(input_data, f)\n","\n","# 저장한 input_data 로드\n","with open('input_data.pkl', 'rb') as f:\n","    loaded_input_data = pickle.load(f)"],"metadata":{"id":"eAZBWCttBCBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장한 input_data 로드\n","with open('input_data.pkl', 'rb') as f:\n","    loaded_input_data = pickle.load(f)"],"metadata":{"id":"R6_B7Fah43sL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","# Colab 파일 다운로드\n","files.download('input_data.pkl')"],"metadata":{"id":"Z9_t0vjXQg-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import os\n","import torchvision.transforms as transforms\n","from torchvision.io import read_image\n","from torchvision.transforms import functional as F\n","\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Final_team_3/Data/archive (2)/Train'\n","\n","input_data = []\n","\n","# 각 라벨에 대해 처리\n","for label in os.listdir(data_path):\n","    label_dir = os.path.join(data_path, label)\n","\n","    for img_file in os.listdir(label_dir):\n","        img_path = os.path.join(label_dir, img_file)\n","\n","        # 파일 이름에서 공통 부분과 프레임 순서 추출\n","        video_name, frame_info = img_file.split(\"_x264_\")\n","        frame_num, _ = os.path.splitext(frame_info)\n","        frame_num = int(frame_num)\n","\n","        # 이미지 데이터 읽어오기\n","        img = Image.open(img_path)\n","\n","        input_data.append((label, video_name, frame_num, img))\n","\n","# SlowFast 모델에 입력으로 활용하기 위한 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),  # 이미지 크기 조정\n","    transforms.ToTensor(),  # 텐서로 변환\n","])\n","\n","# SlowFast 모델의 입력 형식에 맞게 데이터를 처리\n","slow_pathway_data = {}\n","fast_pathway_data = {}\n","for label, video_name, frame_num, img in input_data:\n","    img_tensor = transform(img)\n","    if label not in slow_pathway_data:\n","        slow_pathway_data[label] = []\n","    if label not in fast_pathway_data:\n","        fast_pathway_data[label] = []\n","\n","    if frame_num % 16 == 0:  # Slow pathway\n","        slow_pathway_data[label].append(img_tensor)\n","    else:  # Fast pathway\n","        fast_pathway_data[label].append(img_tensor)\n","\n","# SlowFast 모델에 입력으로 활용할 데이터 생성\n","slowfast_input_data = []\n","for label in slow_pathway_data:\n","    slow_tensors = slow_pathway_data[label]\n","    fast_tensors = fast_pathway_data[label]\n","    for i in range(len(slow_tensors)):\n","        slow_tensor = torch.stack(slow_tensors[i:i+8])  # 8 프레임씩 묶음\n","        fast_tensor = torch.stack(fast_tensors[i:i+8])\n","        slowfast_input_data.append((slow_tensor, fast_tensor, label))\n","\n","\n","\n"],"metadata":{"id":"r7swv906jf3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes = ['Explosion', 'Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Fighting','NormalVideos']"],"metadata":{"id":"QyiJHPR3-9nv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label in classes:\n","    label_dir = os.path.join(data_path, label)\n","    print(label_dir)\n"],"metadata":{"id":"JQBPkAysojA_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dir"],"metadata":{"id":"Gulz1aQu-X53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CR1xCwJy-Y0E"},"execution_count":null,"outputs":[]}]}