# @package optim

resume: false

optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001

lr_scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    gamma: 0.1
    step_size: 15

batch_size: 4
batch_accumulation: 2
epochs: 100

val_freq: 1
val_batch_size: ${.batch_size}
val_device: cuda

num_workers: 16

debug: true
debug_freq: 5
log_every: 1

loss:
    _target_: torch.nn.BCELoss

